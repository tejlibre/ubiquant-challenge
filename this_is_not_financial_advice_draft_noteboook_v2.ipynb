{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "this-is-not-financial-advice-draft-noteboook-v2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejlibre/ubiquant-challenge/blob/main/this_is_not_financial_advice_draft_noteboook_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Competition: Ubiquant Market Prediction\n",
        "## Make predictions against future market data\n",
        "\n",
        "\n",
        "<img src='https://portal.fgv.br/sites/portal.fgv.br/files/styles/noticia_geral/public/noticias/11/21/mercado-financeiro.jpg?itok=fuZgDZBa'>\n",
        "\n",
        "Image courtesy of FGV"
      ],
      "metadata": {
        "id": "SlVlCa4JQLkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goal:\n",
        "In this competition sponsored by Ubiquant, a model needs to be built to forecast an investment's return rate, whereby the training and testing of the algorithm will be carried on anonymised historical prices. If successful, an improvement in the ability of quantitative researchers to forecast returns will be derived from the model built. This will enable investors at any scale to make better decisions.\n",
        "\n",
        "\n",
        "## Context:\n",
        "Ubiquant Investment (Beijing) Co., Ltd is a leading domestic quantitative hedge fund based in China. Established in 2012, they are committed to creating long-term stable returns for investors. Regardless of any investment strategy, fluctuations are expected in the financial market. Despite this variance, professional investors try to estimate their overall returns. Risks and returns differ based on investment types and other factors, which impact stability and volatility. To attempt to predict returns, there are many computer-based algorithms and models for financial market trading. Yet, with new techniques and approaches, data science could improve quantitative researchers' ability to forecast an investment's return.\n",
        "\n",
        "## Team members\n",
        "Nmeso, Antsa, Tejuswi, Akhil"
      ],
      "metadata": {
        "id": "XzEPjIPiQLkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "Submissions are evaluated on the mean of the Pearson correlation coefficient for each time ID.\n",
        "\n",
        "Pearson correlation coefficient (PCC, also referred to as Pearson's r or bivariate correlation) is a measure of linear correlation between two sets of values. The formulae is denoted as follows:\n",
        "\n",
        "$\\huge r_{xy} = \\frac{n\\sum{xy} - \\sum{x} \\sum{y}} {\\sqrt{n \\sum{x^2} - \\left(\\sum{x}\\right)^2} {\\sqrt{n \\sum{y^2} -\\left(\\sum{y}\\right)^2}}}$\n",
        "\n",
        "* $r_{xy}$ = PCC\n",
        "* $n$ = Number of samples\n",
        "* $x$ = First set of values\n",
        "* $y$ = Second set of values\n",
        "\n",
        "As can be seen from the above equation, PCC is the ratio between the covariance of two variables and the product of their standard deviations. The result always has a value between âˆ’1 and 1. As with covariance itself, the measure can only reflect a linear correlation of variables, and ignores many other types of relationship or correlation.\n",
        "\n",
        "Mean of Pearson correlation coefficient across time IDs can be expressed as follows:\n",
        "\n",
        "$\\huge \\textit{Mean} \\quad r_{xy} = {\\frac{1}{T} \\sum_{i=1}^{T}} t_i r_{xy}$\n",
        "\n",
        "* $T$ = Number of time IDs\n",
        "* $t_i r_{xy}$ = ith time ID's PCC\n",
        "[[](http://)](http://)"
      ],
      "metadata": {
        "id": "ea7NLz7iQLkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology:\n",
        "<img src='https://raw.githubusercontent.com/akhil-gun/DSI/master/Pipeline_v2.drawio.png'>\n"
      ],
      "metadata": {
        "id": "N7B5Ky44QLkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General necessary imports"
      ],
      "metadata": {
        "id": "r6zIx7o5QLkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import some necessary packages\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import random\n",
        "import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "from argparse import Namespace\n",
        "import random\n",
        "import os\n",
        "import gc\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# setting up options\n",
        "import warnings\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "warnings.filterwarnings('ignore')\n",
        "from cycler import cycler\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:58:54.640776Z",
          "iopub.execute_input": "2022-03-05T16:58:54.641848Z",
          "iopub.status.idle": "2022-03-05T16:58:55.780855Z",
          "shell.execute_reply.started": "2022-03-05T16:58:54.641795Z",
          "shell.execute_reply": "2022-03-05T16:58:55.78006Z"
        },
        "trusted": true,
        "id": "k5ONfJ2VQLkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "Fn-6fk18QLkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Folder and files given:**\n",
        "\n",
        "* `ubiquant/` - The image delivery API that will serve the test set. You may need Python 3.7 and a Linux environment to run the example test set through the API offline without errors.\n",
        "\n",
        "* `example_sample_submission.csv` - An example submission file provided so the publicly accessible copy of the API provides the correct data shape and format.\n",
        "\n",
        "* `example_test.csv` - Random data provided to demonstrate what shape and format of data the API will deliver to your notebook when you submit.\n",
        "\n",
        "* `train.csv` - Train dataset. Further descibe later in the notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "IvDG2tEFQLkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    seed=21,\n",
        "    folds=5,\n",
        "    workers=4,\n",
        "    samples=2500000,\n",
        "    data_path=Path(\"../input/ubiquant-parquet/\"),\n",
        ")\n",
        "\n",
        "\n",
        "def reduce_mem_usage(df):\n",
        "    '''Function to reduce memory usage of dataframe.\n",
        "    \n",
        "    Reference: https://www.kaggle.com/valleyzw/ubiquant-lgbm-baseline''' \n",
        "\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "\n",
        "train = reduce_mem_usage(pd.read_parquet(args.data_path.joinpath(\"train_low_mem.parquet\")))\n",
        "train = pd.read_parquet(args.data_path.joinpath(\"train_low_mem.parquet\"))\n",
        "\n",
        "\n",
        "FEATURES = [col for col in train.columns if col not in ['target', 'row_id']]\n",
        "NUM_FEATURES = [col for col in train.columns if col not in ['target', 'row_id', 'investment_id', 'time_id']]\n",
        "CAT_FEATURES = [feature for feature in FEATURES if feature not in NUM_FEATURES]\n",
        "\n",
        "inv_ids = random.choices(train['investment_id'].unique(), k=3)\n",
        "\n",
        "#train = pd.read_csv('/kaggle/input/ubiquant-market-prediction/train.csv', nrows=10000)\n",
        "# inv_ids = random.choices(train['investment_id'].unique(), k=3)\n",
        "\n",
        "# train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n",
        "# inv_ids = random.choices(train['investment_id'].unique(), k=3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:28:42.793628Z",
          "iopub.execute_input": "2022-03-05T16:28:42.794328Z",
          "iopub.status.idle": "2022-03-05T16:32:45.048517Z",
          "shell.execute_reply.started": "2022-03-05T16:28:42.794295Z",
          "shell.execute_reply": "2022-03-05T16:32:45.045554Z"
        },
        "trusted": true,
        "id": "QvAQutwAQLkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting an initial overview of dataset's number of rows, columns, types, and memory usage\n",
        "train.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:32:45.053388Z",
          "iopub.execute_input": "2022-03-05T16:32:45.053671Z",
          "iopub.status.idle": "2022-03-05T16:32:45.096959Z",
          "shell.execute_reply.started": "2022-03-05T16:32:45.053634Z",
          "shell.execute_reply": "2022-03-05T16:32:45.096035Z"
        },
        "trusted": true,
        "id": "BYsgNjxQQLkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Columns of train.csv:**\n",
        "\n",
        "* ```row_id```  - A unique identifier for the row.\n",
        "\n",
        "* ```time_id``` - The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.\n",
        "\n",
        "* ```investment_id``` - The ID code for an investment. Not all investment have data in all time IDs.\n",
        "\n",
        "* ```target``` - The target (Anonymized).\n",
        "\n",
        "* ```[f_0:f_299]``` - Anonymized features generated from market data."
      ],
      "metadata": {
        "id": "E5tZ76d9QLkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the first 10 elements of the dataframe, where the colour scale corresponds to a higher value being redder/darker\n",
        "train.head(10).style.background_gradient(cmap='Reds')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:32:45.099805Z",
          "iopub.execute_input": "2022-03-05T16:32:45.100489Z",
          "iopub.status.idle": "2022-03-05T16:32:46.091722Z",
          "shell.execute_reply.started": "2022-03-05T16:32:45.100456Z",
          "shell.execute_reply": "2022-03-05T16:32:46.090956Z"
        },
        "trusted": true,
        "id": "0iR9YTg6QLkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the last 10 elements of the dataframe, where the colour scale corresponds to a higher value being redder/darker\n",
        "train.tail(10).style.background_gradient(cmap='Reds')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:32:46.092778Z",
          "iopub.execute_input": "2022-03-05T16:32:46.09306Z",
          "iopub.status.idle": "2022-03-05T16:32:46.711476Z",
          "shell.execute_reply.started": "2022-03-05T16:32:46.093006Z",
          "shell.execute_reply": "2022-03-05T16:32:46.71085Z"
        },
        "trusted": true,
        "id": "gVyFuiONQLkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Rows in train dataset:', train.shape[0])\n",
        "print('Columns in train dataset:', train.shape[1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:32:46.712874Z",
          "iopub.execute_input": "2022-03-05T16:32:46.713324Z",
          "iopub.status.idle": "2022-03-05T16:32:46.718827Z",
          "shell.execute_reply.started": "2022-03-05T16:32:46.713288Z",
          "shell.execute_reply": "2022-03-05T16:32:46.718214Z"
        },
        "trusted": true,
        "id": "vwJZpjrLQLkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Range of investment ids from {train.investment_id.min()} to {train.investment_id.max()}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:32:46.720116Z",
          "iopub.execute_input": "2022-03-05T16:32:46.720576Z",
          "iopub.status.idle": "2022-03-05T16:32:46.739128Z",
          "shell.execute_reply.started": "2022-03-05T16:32:46.720539Z",
          "shell.execute_reply": "2022-03-05T16:32:46.738048Z"
        },
        "trusted": true,
        "id": "9wIUoCjbQLkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_ids  = train.time_id.nunique()\n",
        "investment_ids = train.investment_id.nunique()\n",
        "print(f\"Number of unique time ids: {time_ids}\")\n",
        "print(f\"Number of unique investment ids: {investment_ids}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:32:46.740602Z",
          "iopub.execute_input": "2022-03-05T16:32:46.74108Z",
          "iopub.status.idle": "2022-03-05T16:32:46.78146Z",
          "shell.execute_reply.started": "2022-03-05T16:32:46.741043Z",
          "shell.execute_reply": "2022-03-05T16:32:46.780692Z"
        },
        "trusted": true,
        "id": "szD1FCTqQLkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In training set, there are 3579 unique investments and private test set will include new unseen investment ids. Furthermore, the investment id range is bigger than the number of investment ids themselves, meaning that some investment ids are repeated but for different time ids."
      ],
      "metadata": {
        "id": "0jhMR3_UQLkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if any of the columns have missing values\n",
        "print('Missing values in train dataset:', sum(train.isnull().sum()))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:32:46.782834Z",
          "iopub.execute_input": "2022-03-05T16:32:46.783104Z",
          "iopub.status.idle": "2022-03-05T16:32:48.896617Z",
          "shell.execute_reply.started": "2022-03-05T16:32:46.783068Z",
          "shell.execute_reply": "2022-03-05T16:32:48.895746Z"
        },
        "trusted": true,
        "id": "nFrYSZCaQLkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting unique values to see if null values are represented by some other value such as median\n",
        "train_unique = train.nunique(axis=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:32:48.900097Z",
          "iopub.execute_input": "2022-03-05T16:32:48.900331Z",
          "iopub.status.idle": "2022-03-05T16:34:17.890271Z",
          "shell.execute_reply.started": "2022-03-05T16:32:48.900299Z",
          "shell.execute_reply": "2022-03-05T16:34:17.889497Z"
        },
        "trusted": true,
        "id": "9TcMeuUcQLkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting previous cell results\n",
        "def plot_unique(nu):\n",
        "    '''Plotting the number of unique values per row_id in a bar chart'''\n",
        "    #nu = train_unique.reset_index().sort_values(by = 0)\n",
        "    nu.columns = ['column name','number of unique entries']\n",
        "\n",
        "    plt.figure(figsize=(40,15))\n",
        "    plt.xticks(rotation=90)\n",
        "    chart = sns.barplot(x='column name', y='number of unique entries', data=nu)\n",
        "\n",
        "    chart.set_title(\"Frequency of unique entries per column\")\n",
        "    chart.axhline(train.shape[0], color = \"red\")\n",
        "    \n",
        "nu = train_unique.reset_index().sort_values(by = 0)\n",
        "plot_unique(nu)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:34:17.891568Z",
          "iopub.execute_input": "2022-03-05T16:34:17.891903Z",
          "iopub.status.idle": "2022-03-05T16:34:22.308969Z",
          "shell.execute_reply.started": "2022-03-05T16:34:17.891865Z",
          "shell.execute_reply": "2022-03-05T16:34:22.308248Z"
        },
        "trusted": true,
        "id": "l1PC6Jl3QLkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the bar chart above, it can be seen that about a quarter of the dataset have less than half the amount of unique values compared to the total number of rows [red line]. This suggests that their values might be more categorical that thought of."
      ],
      "metadata": {
        "id": "V0bHzRUfQLkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving features per mask for further investigation \n",
        "def extracting_nunique(nu, threshold):\n",
        "    \"\"\"Extracting the assets depending on threshold value.\n",
        "    Saving in two separate dataframes. \n",
        "    Taking a random of 25 samples of each \"\"\"\n",
        "    nu = train_unique.reset_index().sort_values(by = 0)\n",
        "    nu.columns = ['column_name','number_of_unique_entries']\n",
        "\n",
        "    #dropping 'time_id', 'investment_id', 'row_id', 'target'\n",
        "    nu = nu[nu['column_name'].str.split('_').str[0] == 'f']\n",
        "    nu.info\n",
        "\n",
        "    mask1 = nu['number_of_unique_entries'] < threshold\n",
        "    mask2 = nu['number_of_unique_entries'] > threshold\n",
        "\n",
        "    #print(nu[mask1])\n",
        "    #nu[mask1].shape[0]\n",
        "\n",
        "    #print(nu[mask2])\n",
        "    #nu[mask2].shape[0]\n",
        "\n",
        "    print('Total number of features: ', nu[mask1].shape[0] + nu[mask2].shape[0])\n",
        "    #print(nu[mask1].shape[0] + nu[mask2].shape[0] == nu.shape[0])\n",
        "    \n",
        "    df_sample1 =  nu[mask1].sample(n=25)\n",
        "    df_sample2 =  nu[mask2].sample(n=25)\n",
        "\n",
        "    #df_sample1.column_name.astype('string')\n",
        "    #df_sample1 = df_sample1.values.tolist()\n",
        "    df_sample1 = df_sample1['column_name'].tolist()\n",
        "    df_sample2 = df_sample2['column_name'].tolist()\n",
        "\n",
        "#     print(df_sample1)\n",
        "#     print(df_sample2)\n",
        "\n",
        "    return df_sample1, df_sample2\n",
        "\n",
        "nu = train_unique.reset_index().sort_values(by = 0)\n",
        "threshold = 2 * 10**6\n",
        "\n",
        "df_sample1, df_sample2 = extracting_nunique(nu, threshold)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:34:22.310303Z",
          "iopub.execute_input": "2022-03-05T16:34:22.310681Z",
          "iopub.status.idle": "2022-03-05T16:34:22.485937Z",
          "shell.execute_reply.started": "2022-03-05T16:34:22.310646Z",
          "shell.execute_reply": "2022-03-05T16:34:22.485216Z"
        },
        "trusted": true,
        "id": "m-LqSlOXQLkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chinese Stock market anlysis\n",
        "\n",
        "Following this [discussion](https://www.kaggle.com/c/ubiquant-market-prediction/discussion/309720) on Kaggle, it was noticed that the historical dataset given might be during the 2015 stock market crash, began  on the 12th of June 2015 and ended in early February 2016.  [[Wikipedia]](https://en.wikipedia.org/wiki/2015%E2%80%932016_Chinese_stock_market_turbulence) [2]"
      ],
      "metadata": {
        "id": "-4dtsJOQQLkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting chinese public holidays from 2014 to 2030.\n",
        "calendar_df = pd.read_csv(\"../input/chineseholidays/holidays_of_china_from_2014_to_2030.csv\", parse_dates=[\"date\"], \n",
        "                          date_parser=lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
        "#display(calendar_df.head())\n",
        "\n",
        "# leave only national holidays\n",
        "calendar_df = calendar_df.loc[(calendar_df.type.isin([\"National holiday\", \"Common local holiday\"]))]\n",
        "#display(calendar_df.head())\n",
        "\n",
        "# fill with everyday from 2014 to 2022\n",
        "calendar_df = (\n",
        "    pd.DataFrame({\"date\": pd.date_range(start=\"2014-01-01\", end=\"2022-01-01\")}).merge(calendar_df, on=\"date\", how=\"left\")\n",
        "    .assign(weekday=lambda x: x.date.dt.day_name(), year=lambda x: x.date.dt.year)\n",
        ")\n",
        "#display(calendar_df.head())\n",
        "\n",
        "# remove weekends and national holidays and align with time_id\n",
        "calendar_df = (\n",
        "    calendar_df.loc[(~calendar_df.weekday.isin([\"Sunday\", \"Saturday\"]))&(calendar_df.name.isna())]\n",
        "    .reset_index(drop=True)\n",
        "    .head(train.time_id.max()+1)\n",
        "    .dropna(axis=1)\n",
        ")\n",
        "#display(calendar_df.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:34:22.48739Z",
          "iopub.execute_input": "2022-03-05T16:34:22.487634Z",
          "iopub.status.idle": "2022-03-05T16:34:22.564191Z",
          "shell.execute_reply.started": "2022-03-05T16:34:22.4876Z",
          "shell.execute_reply": "2022-03-05T16:34:22.563365Z"
        },
        "trusted": true,
        "id": "r7PchLk0QLkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, (ax0, ax1, ax2) = plt.subplots(3, 1, gridspec_kw={'height_ratios': [1,1,6]}, figsize=(10,10), sharex=True, dpi=128)\n",
        "_df = (\n",
        "    train[['time_id', 'investment_id']]\n",
        "    .groupby(\"time_id\")\n",
        "    .count()\n",
        "    .reindex(range(train.time_id.min(), train.time_id.max()+1))\n",
        "    .set_index(calendar_df.date)\n",
        ")\n",
        "peeks, _ = find_peaks(-_df.values.squeeze(), threshold=200)\n",
        "_df.plot(ax=ax0)\n",
        "ax0.set_xticks(ticks=peeks, minor=True)\n",
        "ax0.set_ylabel(\"count\")\n",
        "ax0.legend(loc='upper left')\n",
        "\n",
        "(\n",
        "    train[['time_id', 'target']]\n",
        "    .groupby(\"time_id\")\n",
        "    .mean()\n",
        "    .reindex(range(train.time_id.min(), train.time_id.max()+1))\n",
        "    .set_index(calendar_df.date)\n",
        "    .plot(ax=ax1)\n",
        ")\n",
        "ax1.axvspan(*mdates.date2num(_df.loc[(_df.index>\"2015-06\")&(_df.index<\"2016-03\")].index[[0,-1]]), fill=True, alpha=0.9, color=\"#eee\")\n",
        "ax1.set_ylabel(\"mean\")\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "_df = (\n",
        "    train[['investment_id', 'time_id', \"target\"]]\n",
        "    .pivot_table(index=\"time_id\", columns=\"investment_id\", values=\"target\", aggfunc=\"count\")\n",
        "    .reindex(range(train.time_id.min(), train.time_id.max()+1))\n",
        "    .set_index(calendar_df.date)\n",
        ")\n",
        "ax2.imshow(_df.T, cmap='winter', interpolation='nearest', aspect=\"auto\", origin=\"lower\", alpha=0.6, \n",
        "           extent=[*mdates.date2num([calendar_df.date.min(), calendar_df.date.max()]),train.investment_id.min(), \n",
        "                   train.investment_id.max()])\n",
        "ax2.set_xlabel(\"date\")\n",
        "ax2.set_ylabel(\"investment_id\")\n",
        "ax2.xaxis_date()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:34:22.565723Z",
          "iopub.execute_input": "2022-03-05T16:34:22.56597Z",
          "iopub.status.idle": "2022-03-05T16:34:26.323692Z",
          "shell.execute_reply.started": "2022-03-05T16:34:22.565937Z",
          "shell.execute_reply": "2022-03-05T16:34:26.322864Z"
        },
        "trusted": true,
        "id": "4FpH14jNQLkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Features exploration\n",
        "\n",
        "There are 300 anonymized continuous features in dataset and they are named from f_0 to f_299. \n",
        "\n",
        "All of the features are zero-centered and they have standard deviation of one since they are standardized during the anonymisation process. Most of the features have symmetrical normal distributions but some of them have very extreme outliers which are skewing their distributions.\n",
        "\n",
        "Feature means and standard deviations vary between different time IDs and investments. It looks like feature means and standard deviations are dependent to time. They make sharp transitions on some periods. Feature standard deviations are more likely to make sharp transitions on different periods however feature mean outliers are observed in the same period most of the time. Feature means and standard deviations per investment looks randomly distributed among investments because it is related to those investment's time IDs."
      ],
      "metadata": {
        "id": "AG14b60KQLkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining function to visualise set of 25 features in 5 x 5 grid\n",
        "\n",
        "# features = list(train.columns[4:29])    #   0:24\n",
        "# # features = list(train.columns[29:54])   #  25:49\n",
        "# # features = list(train.columns[54:79])   #  50:74\n",
        "# # features = list(train.columns[79:104])  #  75:99\n",
        "# # features = list(train.columns[104:129]) # 100:124\n",
        "# # features = list(train.columns[129:154]) # 125:149\n",
        "# # features = list(train.columns[154:179]) # 150:174\n",
        "# # features = list(train.columns[179:204]) # 175:199\n",
        "# # features = list(train.columns[204:229]) # 200:224\n",
        "# # features = list(train.columns[229:254]) # 225:249\n",
        "# # features = list(train.columns[254:279]) # 250:274\n",
        "# # features = list(train.columns[279:304]) # 275:300\n",
        "\n",
        "def visualize_feature(features):\n",
        "    plt.rcParams['figure.dpi'] = 600\n",
        "    fig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\n",
        "    gs = fig.add_gridspec(5, 5)\n",
        "    gs.update(wspace=0.3, hspace=0.3)\n",
        "    background_color = '#f6f5f5'\n",
        "    run_no = 0\n",
        "\n",
        "    colormap = ['#1DBA94','#1C5ED2', '#FFC300', '#C70039']\n",
        "    plt.rc('axes', prop_cycle=(cycler('color', colormap)))\n",
        "\n",
        "    for row in range(0, 5):\n",
        "        for col in range(0, 5):\n",
        "            locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n",
        "            locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n",
        "            for s in [\"top\",\"right\"]:\n",
        "                locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n",
        "            run_no += 1  \n",
        "\n",
        "\n",
        "    # Kernel density plots to visualise distribution of each features\n",
        "    run_no = 0\n",
        "    for col in features:\n",
        "        sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train[col], zorder=2, alpha=1, linewidth=1, color='#ffd514')\n",
        "        sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=train[train['investment_id'].isin(inv_ids)][col], \n",
        "                    hue=train[train['investment_id'].isin(inv_ids)]['investment_id'],zorder=2, alpha=1, fill=True, \n",
        "                    color=colormap, linewidth=0.5, legend=False,palette=colormap[:3], hue_order=inv_ids.sort(reverse=True))\n",
        "\n",
        "        locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n",
        "        locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n",
        "        locals()[\"ax\"+str(run_no)].set_ylabel('')\n",
        "        locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n",
        "        locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n",
        "        locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n",
        "        locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n",
        "        #locals()[\"ax\"+str(run_no)].get_legend().remove()\n",
        "\n",
        "        run_no += 1\n",
        "\n",
        "    plt.show()\n",
        "    \n",
        "    return\n",
        "\n",
        "\n",
        "# Choose series of 25 features to graph \n",
        "features = list(df_sample1)\n",
        "visualize_feature(features)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:34:26.325393Z",
          "iopub.execute_input": "2022-03-05T16:34:26.325754Z",
          "iopub.status.idle": "2022-03-05T16:38:37.609635Z",
          "shell.execute_reply.started": "2022-03-05T16:34:26.325716Z",
          "shell.execute_reply": "2022-03-05T16:38:37.608834Z"
        },
        "trusted": true,
        "id": "ykh9gHzDQLkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose series of 25 features to graph \n",
        "features = list(df_sample2)\n",
        "visualize_feature(features)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:38:37.61106Z",
          "iopub.execute_input": "2022-03-05T16:38:37.611462Z",
          "iopub.status.idle": "2022-03-05T16:42:46.533399Z",
          "shell.execute_reply.started": "2022-03-05T16:38:37.611425Z",
          "shell.execute_reply": "2022-03-05T16:42:46.531817Z"
        },
        "trusted": true,
        "id": "6l0LE4htQLkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del train, nu, df_sample1, df_sample2, features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:42:46.534578Z",
          "iopub.execute_input": "2022-03-05T16:42:46.535091Z",
          "iopub.status.idle": "2022-03-05T16:42:46.539589Z",
          "shell.execute_reply.started": "2022-03-05T16:42:46.535055Z",
          "shell.execute_reply": "2022-03-05T16:42:46.53871Z"
        },
        "trusted": true,
        "id": "mRVycKHYQLkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing outliers\n",
        "\n",
        "## using quantile\n",
        "if you have analysis residual of you model, you can saw that he badly approximate outside ~ 5 and 95 quantile.\n",
        "In addition some work clarified, that it is problem related to disbalance investmend_ids on time_ids.\n",
        "If you run a regular catboost baseline, you will probably run into this problem.\n",
        "Let's see what features depend on outside 5 and 95 quantiles.\n",
        "\n",
        "references: https://www.kaggle.com/lucamassaron/eda-target-analysis"
      ],
      "metadata": {
        "id": "fofKk7bUQLkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/kaggle/input/ubiquant-market-prediction/train.csv', nrows=30000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:59:22.117558Z",
          "iopub.execute_input": "2022-03-05T16:59:22.117855Z",
          "iopub.status.idle": "2022-03-05T16:59:25.884992Z",
          "shell.execute_reply.started": "2022-03-05T16:59:22.117821Z",
          "shell.execute_reply": "2022-03-05T16:59:25.884166Z"
        },
        "trusted": true,
        "id": "UFrUZ1ZbQLkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train['target'].count())\n",
        "threshold_right = train['target'].quantile(0.95)\n",
        "threshold_left = train['target'].quantile(0.05)\n",
        "\n",
        "print(threshold_right)\n",
        "print(threshold_left)\n",
        "\n",
        "train = train[(train['target'] > threshold_left) & (train['target'] < threshold_right)]\n",
        "print(train['target'].count())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:59:47.144663Z",
          "iopub.execute_input": "2022-03-05T16:59:47.144948Z",
          "iopub.status.idle": "2022-03-05T16:59:47.195912Z",
          "shell.execute_reply.started": "2022-03-05T16:59:47.144916Z",
          "shell.execute_reply": "2022-03-05T16:59:47.194946Z"
        },
        "trusted": true,
        "id": "_gmTf2HYQLkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_x = train.drop(['target'], axis =1)\n",
        "df_y = train['target'] "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:59:49.29655Z",
          "iopub.execute_input": "2022-03-05T16:59:49.297209Z",
          "iopub.status.idle": "2022-03-05T16:59:49.327638Z",
          "shell.execute_reply.started": "2022-03-05T16:59:49.297159Z",
          "shell.execute_reply": "2022-03-05T16:59:49.326702Z"
        },
        "trusted": true,
        "id": "HYDfNDhiQLkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the dataset\n",
        "\n",
        "X_test_model, y_test_model is the testing set to be used at the end of model training for evaluating model predictions\n",
        "\n",
        "X_train, X_test, y_train, y_test : this is the training set and validation set for feature selection.\n",
        "It will be recombined and used as training set for the model\n",
        "\n"
      ],
      "metadata": {
        "id": "YN_DTpS3QLkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_model, X_test_model, y_train_model, y_test_model = train_test_split(df_x, df_y, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split( X_train_model, y_train_model, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:59:50.98118Z",
          "iopub.execute_input": "2022-03-05T16:59:50.982163Z",
          "iopub.status.idle": "2022-03-05T16:59:51.087761Z",
          "shell.execute_reply.started": "2022-03-05T16:59:50.982121Z",
          "shell.execute_reply": "2022-03-05T16:59:51.086928Z"
        },
        "trusted": true,
        "id": "VM0qUm-_QLkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time series split"
      ],
      "metadata": {
        "id": "d7GbBS4mQLkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acknowledgements\n",
        "\n",
        "https://www.kaggle.com/ionanicleoid/intro-to-comparing-ml-models-lgbm-tuning\n",
        "\n",
        "https://www.kaggle.com/robikscube/ubiquant-parquet\n",
        "\n",
        "https://www.kaggle.com/melanie7744/understanding-the-submission-api-for-newbies\n",
        "\n",
        "Testing a first random forest model\n",
        "To be used later for feature importance"
      ],
      "metadata": {
        "id": "M6D0BpvZQLkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import data"
      ],
      "metadata": {
        "id": "yy47-FPgQLkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import time\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import time\n",
        "\n",
        "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:56:50.344437Z",
          "iopub.execute_input": "2022-03-05T16:56:50.345311Z",
          "iopub.status.idle": "2022-03-05T16:56:50.352309Z",
          "shell.execute_reply.started": "2022-03-05T16:56:50.345266Z",
          "shell.execute_reply": "2022-03-05T16:56:50.351511Z"
        },
        "trusted": true,
        "id": "9P4m6t8CQLkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n",
        "example_test = pd.read_csv('/kaggle/input/ubiquant-market-prediction/example_test.csv')\n",
        "example_sample_submission =  pd.read_csv('/kaggle/input/ubiquant-market-prediction/example_sample_submission.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:56:52.630027Z",
          "iopub.execute_input": "2022-03-05T16:56:52.630629Z",
          "iopub.status.idle": "2022-03-05T16:57:07.147151Z",
          "shell.execute_reply.started": "2022-03-05T16:56:52.630589Z",
          "shell.execute_reply": "2022-03-05T16:57:07.146336Z"
        },
        "trusted": true,
        "id": "JDFPKpwTQLkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [f'f_{i}' for i in range(300)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:57:07.149388Z",
          "iopub.execute_input": "2022-03-05T16:57:07.150076Z",
          "iopub.status.idle": "2022-03-05T16:57:07.154557Z",
          "shell.execute_reply.started": "2022-03-05T16:57:07.15003Z",
          "shell.execute_reply": "2022-03-05T16:57:07.153547Z"
        },
        "trusted": true,
        "id": "6VV5cFr-QLkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DROP_BEFORE = 600\n",
        "\n",
        "df = df[df[\"time_id\"] > DROP_BEFORE].reset_index(drop=True)\n",
        "df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:57:07.156059Z",
          "iopub.execute_input": "2022-03-05T16:57:07.156334Z",
          "iopub.status.idle": "2022-03-05T16:57:13.38371Z",
          "shell.execute_reply.started": "2022-03-05T16:57:07.156296Z",
          "shell.execute_reply": "2022-03-05T16:57:13.38288Z"
        },
        "trusted": true,
        "id": "YYhtCCM1QLkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(df.drop(['target'], axis = 1))\n",
        "y = np.array(df['target'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:57:13.385692Z",
          "iopub.execute_input": "2022-03-05T16:57:13.386149Z",
          "iopub.status.idle": "2022-03-05T16:57:17.852202Z",
          "shell.execute_reply.started": "2022-03-05T16:57:13.386103Z",
          "shell.execute_reply": "2022-03-05T16:57:17.851388Z"
        },
        "trusted": true,
        "id": "u-Pa_wGbQLkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "\n",
        "def time_series_split(n,train_size):\n",
        "    \"\"\"\n",
        "    Time Series cross-validator\n",
        "    Provides train/test indices to split time series data samples\n",
        "    that are observed at fixed time intervals, in train/test sets.\n",
        "    In each split, test indices must be higher than before, and\n",
        "    thus shuffling in cross validator is inappropriate.\n",
        "\n",
        "    This cross-validation object is a variation of KFold.\n",
        "    In the kth split, it returns first k folds as train set and\n",
        "    the (k+1)th fold as test set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n, default=5\n",
        "        Number of splits. Must be at least 2.\n",
        "    max_train_size(int), default=None\n",
        "        Maximum size for a single training set.\n",
        "    Returns\n",
        "    ----------\n",
        "    \n",
        "    References\n",
        "    ----------\n",
        "    .. [1] https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html\n",
        "    \"\"\"\n",
        "    \n",
        "    ts_cv = TimeSeriesSplit(\n",
        "        n_splits=5,\n",
        "        max_train_size=train_size,\n",
        "        test_size=200000,\n",
        "        gap=200000\n",
        "    )\n",
        "    \n",
        "    return ts_cv\n",
        "\n",
        "ts_cv = time_series_split(5,1000000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:57:17.853773Z",
          "iopub.execute_input": "2022-03-05T16:57:17.854057Z",
          "iopub.status.idle": "2022-03-05T16:57:17.861261Z",
          "shell.execute_reply.started": "2022-03-05T16:57:17.854017Z",
          "shell.execute_reply": "2022-03-05T16:57:17.860276Z"
        },
        "trusted": true,
        "id": "uIwS87ZRQLkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=pd.DataFrame(X)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:57:28.368924Z",
          "iopub.execute_input": "2022-03-05T16:57:28.369714Z",
          "iopub.status.idle": "2022-03-05T16:57:28.374072Z",
          "shell.execute_reply.started": "2022-03-05T16:57:28.369669Z",
          "shell.execute_reply": "2022-03-05T16:57:28.373212Z"
        },
        "trusted": true,
        "id": "J192eNW7QLka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits = list(ts_cv.split(X, y))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:57:30.397189Z",
          "iopub.execute_input": "2022-03-05T16:57:30.398296Z",
          "iopub.status.idle": "2022-03-05T16:57:30.412474Z",
          "shell.execute_reply.started": "2022-03-05T16:57:30.398239Z",
          "shell.execute_reply": "2022-03-05T16:57:30.411424Z"
        },
        "trusted": true,
        "id": "EphPlnbbQLka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = [f\"feature {i}\" for i in range(X.shape[1])]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:57:31.885085Z",
          "iopub.execute_input": "2022-03-05T16:57:31.885833Z",
          "iopub.status.idle": "2022-03-05T16:57:31.889667Z",
          "shell.execute_reply.started": "2022-03-05T16:57:31.885791Z",
          "shell.execute_reply": "2022-03-05T16:57:31.888962Z"
        },
        "trusted": true,
        "id": "ZNvdbi1VQLka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "for fold in all_splits:\n",
        "    xtrain, ytrain = x.iloc[fold[0]], df.iloc[fold[0]]['target']\n",
        "    xval, yval = x.iloc[fold[1]], df.iloc[fold[1]]['target']\n",
        "    x_train.append(xtrain)\n",
        "    y_train.append(ytrain)\n",
        "    x_test.append(xval)\n",
        "    y_test.append(yval)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:58:02.760942Z",
          "iopub.execute_input": "2022-03-05T16:58:02.761704Z"
        },
        "trusted": true,
        "id": "3st3xXo9QLka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "#to save it\n",
        "with open(\"train_time_split.pkl\", \"wb\") as f:\n",
        "    pkl.dump([x_train, y_train, x_test, y_test], f)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:58:29.583363Z",
          "iopub.status.idle": "2022-03-05T16:58:29.583912Z",
          "shell.execute_reply.started": "2022-03-05T16:58:29.583646Z",
          "shell.execute_reply": "2022-03-05T16:58:29.583673Z"
        },
        "trusted": true,
        "id": "pxNhPo-XQLkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to load it\n",
        "#with open(\"train_time_split.pkl\", \"rb\") as f:\n",
        "#    x_train, y_train, x_test, y_test = pkl.load(f)"
      ],
      "metadata": {
        "trusted": true,
        "id": "X3xMFhq4QLkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying scaling\n",
        "\n",
        "### Acknowledgements\n",
        "1. https://www.kaggle.com/kartushovdanil/ubiquant-market-prediction-eda#4.-Features\n",
        "2. https://www.kaggle.com/valleyzw/ubiquant-lgbm-baseline\n",
        "3. https://www.kaggle.com/ilialar/ubiquant-eda-and-baseline"
      ],
      "metadata": {
        "id": "BGq7juITQLkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Separate Normally and Skewed Distributed Features\n",
        "normal_features = [1 , 2 , 6 , 9 , 20 , 21, 24\n",
        "                  ,28 , 35 , 36 , 40 , 43 ,\n",
        "                  50 , 51 , 57 , 67 , 69 , 72,\n",
        "                  75 , 76 ,82 ,85 , 86 ,\n",
        "                  90 , 93 , 94 , 96 , 98 ,\n",
        "                  103 , 105 , 109 , 106 , 114 ,116,\n",
        "                  125 , 126 , 130 , 133 , 134 ,135 ,139,140 , 141 ,144,146,\n",
        "                  141 ,171,\n",
        "                  178 , 180 , 185 , 189 , 192 , 194 ,195 ,199,\n",
        "                  205 , 206 ,212 ,213 , 217 , 221 ,222,223,\n",
        "                  226 , 230 , 239 ,242 ,\n",
        "                   252 , 254 ,256 , 259 ,261 , 266 ,273,\n",
        "                  276 , 283 , 285 , 290 , 297]\n",
        "\n",
        "\n",
        "normal_features_list = ['f_'+str(i) for i in normal_features]\n",
        "\n",
        "\n",
        "print(\"Number of Normally dist features approx are\",len(normal_features_list))\n",
        "\n",
        "all_features_list = ['f_'+str(i) for i in range(0 , 300)]\n",
        "skewed_features_list = list(set(all_features_list).difference(set(normal_features_list)))\n",
        "print(\"Number of Skewed dist features approx are\",len(skewed_features_list))\n",
        "## Check for mistakes\n",
        "print(set(skewed_features_list).intersection(set(normal_features_list)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:59:31.974958Z",
          "iopub.execute_input": "2022-03-05T16:59:31.975586Z",
          "iopub.status.idle": "2022-03-05T16:59:31.999634Z",
          "shell.execute_reply.started": "2022-03-05T16:59:31.975543Z",
          "shell.execute_reply": "2022-03-05T16:59:31.998917Z"
        },
        "trusted": true,
        "id": "7d6x8H7CQLkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_normal = StandardScaler()\n",
        "scaler_outlier = RobustScaler()\n",
        "\n",
        "X_train[normal_features_list] = scaler_normal.fit_transform(X_train[normal_features_list])\n",
        "X_train[skewed_features_list] = scaler_outlier.fit_transform(X_train[skewed_features_list])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T16:59:59.020646Z",
          "iopub.execute_input": "2022-03-05T16:59:59.021342Z",
          "iopub.status.idle": "2022-03-05T16:59:59.283706Z",
          "shell.execute_reply.started": "2022-03-05T16:59:59.021282Z",
          "shell.execute_reply": "2022-03-05T16:59:59.281297Z"
        },
        "trusted": true,
        "id": "e-h-dvQfQLkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:05.782701Z",
          "iopub.execute_input": "2022-03-05T17:00:05.783166Z",
          "iopub.status.idle": "2022-03-05T17:00:05.999569Z",
          "shell.execute_reply.started": "2022-03-05T17:00:05.78312Z",
          "shell.execute_reply": "2022-03-05T17:00:05.998756Z"
        },
        "trusted": true,
        "id": "neSTclUsQLkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Always apply scaler transform on validation data separately\n",
        "X_test[normal_features_list] = scaler_normal.transform(X_test[normal_features_list])\n",
        "X_test[skewed_features_list] = scaler_outlier.transform(X_test[skewed_features_list])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:09.6085Z",
          "iopub.execute_input": "2022-03-05T17:00:09.609049Z",
          "iopub.status.idle": "2022-03-05T17:00:09.648594Z",
          "shell.execute_reply.started": "2022-03-05T17:00:09.608988Z",
          "shell.execute_reply": "2022-03-05T17:00:09.647769Z"
        },
        "trusted": true,
        "id": "mVAJqFOrQLkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Order reduction using PCA\n",
        "\n",
        "From our litterature review, we found that combining PCA with ANN gives better accuracy in the prediction. In this notebook, we are implementing PCA to make a feature selection for our data.\n",
        "\n",
        "PCA gives the best possible representation of a dataset while reducing the dimension. This transformation our data in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component, in turn, has the highest possible variance possible.\n",
        "\n",
        "Some of the codes were taken from [this notebook](https://www.kaggle.com/ollibolli/lgb-pca-train-5-fold) and this [other one](https://www.kaggle.com/miguelangelnieto/pca-and-regression). We also read [this blogpost](https://www.reneshbedre.com/blog/principal-component-analysis.html).\n",
        "\n",
        "The processing of the data follows from [here](https://www.kaggle.com/nmesoegwuekwe/preprocessings)\n",
        "\n",
        "Goals :\n",
        "\n",
        "Feature reduction with PCA\n",
        "Data transformation  \n",
        "Test different regression models to illustrate the difference."
      ],
      "metadata": {
        "id": "Q-fe3f8FQLkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making an instance of the PCA class and fit it to our data."
      ],
      "metadata": {
        "id": "q1McjHpZQLkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First, we will work without the time_id as feature and see what happens."
      ],
      "metadata": {
        "id": "60S9TpfkQLkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_out = PCA().fit(X_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:16.901273Z",
          "iopub.execute_input": "2022-03-05T17:00:16.901767Z",
          "iopub.status.idle": "2022-03-05T17:00:18.710465Z",
          "shell.execute_reply.started": "2022-03-05T17:00:16.901715Z",
          "shell.execute_reply": "2022-03-05T17:00:18.709479Z"
        },
        "trusted": true,
        "id": "-nMariNqQLkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proportion of variance\n",
        "\n",
        "\n",
        "In this part, we are viewing how much each principal component contributes to the variance of our dataset."
      ],
      "metadata": {
        "id": "OFpAOUxpQLkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_out.explained_variance_ratio_ "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:18.717189Z",
          "iopub.execute_input": "2022-03-05T17:00:18.717611Z",
          "iopub.status.idle": "2022-03-05T17:00:18.738192Z",
          "shell.execute_reply.started": "2022-03-05T17:00:18.717562Z",
          "shell.execute_reply": "2022-03-05T17:00:18.737204Z"
        },
        "trusted": true,
        "id": "tr93rephQLkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cummulative proportion of variance\n",
        "\n",
        "Now, let us cumulate the proportion to see which combination of the PCs contribute mostly in the variance. That will helps us to be able the PCs with low information and hence, reduce the dimension."
      ],
      "metadata": {
        "id": "ehP11HihQLkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cumsum = np.cumsum(pca_out.explained_variance_ratio_)\n",
        "cumsum"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:18.739534Z",
          "iopub.execute_input": "2022-03-05T17:00:18.744059Z",
          "iopub.status.idle": "2022-03-05T17:00:18.76228Z",
          "shell.execute_reply.started": "2022-03-05T17:00:18.743985Z",
          "shell.execute_reply": "2022-03-05T17:00:18.761104Z"
        },
        "trusted": true,
        "id": "FM2KL9C8QLke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# component loadings or weights (correlation coefficient between original variables and the component) \n",
        "# component loadings represents the elements of the eigenvector\n",
        "# the squared loadings within the PCs always sums to 1\n",
        "loadings = pca_out.components_\n",
        "num_pc = pca_out.n_features_\n",
        "pc_list = [\"PC\"+str(i) for i in list(range(1, num_pc+1))]\n",
        "loadings_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))\n",
        "loadings_df['variable'] = X_train.columns.values\n",
        "loadings_df = loadings_df.set_index('variable')\n",
        "loadings_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:18.763974Z",
          "iopub.execute_input": "2022-03-05T17:00:18.764585Z",
          "iopub.status.idle": "2022-03-05T17:00:19.040982Z",
          "shell.execute_reply.started": "2022-03-05T17:00:18.764539Z",
          "shell.execute_reply": "2022-03-05T17:00:19.040289Z"
        },
        "trusted": true,
        "id": "QFDCglw-QLke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCs retention\n",
        "We will only keep the PCS that explain the most variance, in our case, we will take only 90 % (it is very subjective). "
      ],
      "metadata": {
        "id": "c_KMQc5PQLke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_to_retain(threshold) :\n",
        "    '''This function take as input a threshold t and return the number of PCs contributes to the proportion t of the variance'''\n",
        "    for i in range(len(cumsum)) :\n",
        "        if cumsum[i]>threshold :\n",
        "            break\n",
        "    return i"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:19.043456Z",
          "iopub.execute_input": "2022-03-05T17:00:19.043935Z",
          "iopub.status.idle": "2022-03-05T17:00:19.049777Z",
          "shell.execute_reply.started": "2022-03-05T17:00:19.043894Z",
          "shell.execute_reply": "2022-03-05T17:00:19.049068Z"
        },
        "trusted": true,
        "id": "JV6Qjn2CQLke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transforming the datasets\n",
        "\n",
        "Now that we decide which combination of PCs to keep, we need to transform our training dataset. If we want to use the PCA in a model, we need also to apply the same transformation to the test dataset."
      ],
      "metadata": {
        "id": "KWeQdeoJQLke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(dataset, pca_model = pca_out, num_pca = 100) :\n",
        "    '''This function transforms the dataset using the trained PCA model'''\n",
        "    return pca_model.transform(dataset)[:,:num_pca ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:19.051204Z",
          "iopub.execute_input": "2022-03-05T17:00:19.05161Z",
          "iopub.status.idle": "2022-03-05T17:00:19.059725Z",
          "shell.execute_reply.started": "2022-03-05T17:00:19.051571Z",
          "shell.execute_reply": "2022-03-05T17:00:19.059035Z"
        },
        "trusted": true,
        "id": "VWV0xNCfQLkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_PCA = transform(X_train) \n",
        "\n",
        "X_test_PCA = transform(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:19.061357Z",
          "iopub.execute_input": "2022-03-05T17:00:19.06185Z",
          "iopub.status.idle": "2022-03-05T17:00:20.138053Z",
          "shell.execute_reply.started": "2022-03-05T17:00:19.061743Z",
          "shell.execute_reply": "2022-03-05T17:00:20.137105Z"
        },
        "trusted": true,
        "id": "D0eMtRuIQLkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Viewing the new PCs dataset.\n",
        "\n",
        "Let us see how the transformed dataset looks like."
      ],
      "metadata": {
        "id": "Q0Fo9sWDQLkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X_train_PCA).head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:20.142931Z",
          "iopub.execute_input": "2022-03-05T17:00:20.144015Z",
          "iopub.status.idle": "2022-03-05T17:00:20.269837Z",
          "shell.execute_reply.started": "2022-03-05T17:00:20.143946Z",
          "shell.execute_reply": "2022-03-05T17:00:20.268893Z"
        },
        "trusted": true,
        "id": "RvvC1XFkQLkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here we can import the transformed dataset in a new file**\n",
        "\n"
      ],
      "metadata": {
        "id": "OHoomyYRQLkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del loadings_df # to free-up memory"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:20.271465Z",
          "iopub.execute_input": "2022-03-05T17:00:20.271777Z",
          "iopub.status.idle": "2022-03-05T17:00:20.275915Z",
          "shell.execute_reply.started": "2022-03-05T17:00:20.271735Z",
          "shell.execute_reply": "2022-03-05T17:00:20.275096Z"
        },
        "trusted": true,
        "id": "3jX1L0UKQLkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ### Working with the original data, without PCA."
      ],
      "metadata": {
        "id": "Wtlo9wElQLkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LGBM for feature selection\n",
        "\n",
        "LGBM stands for Light Gradient Boosting Machine. LGBM is a fast and efficient gradient boosting framework based on decision tree algorithm. LGBM is a similar boosting method to XGBoost but is especially different in how it creates the tree or base learners. \n",
        "\n",
        "Unlike other ensemble techniques, LGBM grows trees leaf-wise, which can reduce loss during the sequential boosting process. This usually results in higher accuracy than other boosting algorithms. "
      ],
      "metadata": {
        "id": "wmjpu37pQLkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import some necessary packages"
      ],
      "metadata": {
        "id": "0xax6PijQLkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import random\n",
        "import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "from argparse import Namespace\n",
        "import random\n",
        "import os\n",
        "import gc\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import pickle\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "from lightgbm import early_stopping, log_evaluation, record_evaluation\n",
        "print(\"LightGBM version:  {}\".format(lgb.__version__))\n",
        "\n",
        "# setting up options\n",
        "import warnings\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "warnings.filterwarnings('ignore')\n",
        "from cycler import cycler\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "!python --version"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:20.278089Z",
          "iopub.execute_input": "2022-03-05T17:00:20.278419Z",
          "iopub.status.idle": "2022-03-05T17:00:23.442218Z",
          "shell.execute_reply.started": "2022-03-05T17:00:20.278361Z",
          "shell.execute_reply": "2022-03-05T17:00:23.441212Z"
        },
        "trusted": true,
        "id": "3stOLo6JQLkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset"
      ],
      "metadata": {
        "id": "Jk-jkEQwQLkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n",
        "\n",
        "display(df_train.shape)\n",
        "display(df_train.info())\n",
        "df_train.head()\n",
        "#df_train.tail()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:23.444393Z",
          "iopub.execute_input": "2022-03-05T17:00:23.445032Z",
          "iopub.status.idle": "2022-03-05T17:00:40.360303Z",
          "shell.execute_reply.started": "2022-03-05T17:00:23.444967Z",
          "shell.execute_reply": "2022-03-05T17:00:40.359149Z"
        },
        "trusted": true,
        "id": "T6qivQZ3QLkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic dataset preprocessing\n",
        "\n",
        "Filtering the dataset, whereby assets after time_id of 600 are used (after the gap in the data found in the EDA). "
      ],
      "metadata": {
        "id": "8mafSXamQLkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_time_id_to_use = 600 \n",
        "features_to_use= [col for col in df_train.columns if col.startswith(\"f\")] # use only the anonymised features\n",
        "time_id_to_split_train_and_val = 1000\n",
        "\n",
        "\n",
        "df_train = df_train.loc[df_train.time_id >= first_time_id_to_use]\n",
        "print(\"df_train.shape: \",df_train.shape)\n",
        "\n",
        "X_train = df_train.loc[df_train.time_id < time_id_to_split_train_and_val]\n",
        "X_val = df_train.loc[df_train.time_id >= time_id_to_split_train_and_val]\n",
        "y_train = X_train.target\n",
        "y_val = X_val.target\n",
        "X_train = X_train[features_to_use]\n",
        "X_val = X_val[features_to_use]\n",
        "print(\"X_train.shape:  \", X_train.shape)\n",
        "print(\"X_val.shape:    \", X_val.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:40.361946Z",
          "iopub.execute_input": "2022-03-05T17:00:40.362308Z",
          "iopub.status.idle": "2022-03-05T17:00:51.794491Z",
          "shell.execute_reply.started": "2022-03-05T17:00:40.36227Z",
          "shell.execute_reply": "2022-03-05T17:00:51.793592Z"
        },
        "trusted": true,
        "id": "xrykxaxyQLkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df_train #free up memory\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:51.796052Z",
          "iopub.execute_input": "2022-03-05T17:00:51.796528Z",
          "iopub.status.idle": "2022-03-05T17:00:52.025801Z",
          "shell.execute_reply.started": "2022-03-05T17:00:51.796488Z",
          "shell.execute_reply": "2022-03-05T17:00:52.024882Z"
        },
        "trusted": true,
        "id": "sxSg7cWzQLkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the LGBM model"
      ],
      "metadata": {
        "id": "T0dPWcBdQLkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LBGM_model(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Implement Scikit-learn's LGBM model\n",
        "    Also returns useful evaluation metrics such as MSE, RMSE, Pearson's coefficient, \n",
        "    and execution time of the model.\n",
        "    \n",
        "    Read more in the :ref:`Docs <https://lightgbm.readthedocs.io/en/latest/Python-Intro.html#training>`.\n",
        "   \n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : pandas dataframe\n",
        "        Features subset of dataset used for training.\n",
        "    y_train : pandas dataframe\n",
        "        Target of training subset.\n",
        "    X_val : pandas series\n",
        "        Features subset of dataset used for validation/testing.\n",
        "    y_val : pandas series  \n",
        "        Target of validation/testing subset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model:\n",
        "    \n",
        "    metric_over_time: Dictionary\n",
        "        Dictionary of evalution of metrics.\n",
        "    mse: float\n",
        "        Mean-squared error.\n",
        "    rmse: float\n",
        "        Root-mean squared error.\n",
        "    pcc: float\n",
        "        Pearson correlation coefficient.\n",
        "    execution_time: float\n",
        "        Time for model to execute [s].\n",
        "    References\n",
        "    ----------\n",
        "       [1] `Microsoft Corporation, (2000). LightGBM Python-package introduction\n",
        "       <https://lightgbm.readthedocs.io/en/latest/Python-Intro.html#training>`_.\n",
        "       [2] `Scikit-learn, (2021). sklearn.metrics.mean_squared_error\n",
        "       <https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html>`_.\n",
        "    \"\"\"\n",
        "    # Create LGBM datasets\n",
        "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
        "    dval = lgb.Dataset(X_val, label=y_val)\n",
        "    \n",
        "    # Parameters not finetuned yet\n",
        "    lgb_params = {'objective': 'regression',\n",
        "        'metric': 'MSE',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'lambda_l1': 2.3e-05,\n",
        "        'lambda_l2': 0.1,\n",
        "        'num_leaves': 4,\n",
        "        'feature_fraction': 0.5,\n",
        "        'bagging_fraction': 0.9,\n",
        "        'bagging_freq': 7,\n",
        "        'min_child_samples': 20,\n",
        "        'num_iterations': 1000\n",
        "                 }\n",
        "    \n",
        "    ts = time.time() # Initialising time for measuring execution time\n",
        "\n",
        "    metric_over_time = {} # Dict for logging the evaluation metrics\n",
        "\n",
        "    model = lgb.train(        \n",
        "            lgb_params, \n",
        "            dtrain, \n",
        "            valid_sets=[dtrain, dval],\n",
        "            valid_names=['Train','Validation'],\n",
        "            callbacks=[early_stopping(100), log_evaluation(100), record_evaluation(metric_over_time)]\n",
        "        )\n",
        "\n",
        "    execution_time = time.time() - ts\n",
        "    \n",
        "    print(\"\\nTraining time of model: \" + str(round(execution_time, 3)) + \"s\")\n",
        "    \n",
        "    y_val_hat = model.predict(X_val)\n",
        "\n",
        "    mse = mean_squared_error(y_val, y_val_hat, squared=True)\n",
        "    rmse = mean_squared_error(y_val, y_val_hat, squared=False)\n",
        "\n",
        "    print('\\nEvalution on val data:')\n",
        "    # Using MSE as a proxy for pearson correlation (https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302181)\n",
        "    print(\"MSE:  \", round(mse, 5))\n",
        "    print(\"RMSE: \", round(rmse, 5))\n",
        "    \n",
        "    # Getting Pearson's correlation coeff. \n",
        "    pcc, _ = pearsonr(y_val_hat, y_val)\n",
        "    #print(pearsonr(y_val_hat, y_val))\n",
        "    print(\"PCC on validation data: \", round(pcc, 5))\n",
        "    \n",
        "    # Plotting metric over time\n",
        "    lgb.plot_metric(metric_over_time, figsize=(20,10))\n",
        "    plt.show()\n",
        "    \n",
        "    return model, metric_over_time, mse, rmse, pcc, execution_time  \n",
        "    "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:52.031173Z",
          "iopub.execute_input": "2022-03-05T17:00:52.031391Z",
          "iopub.status.idle": "2022-03-05T17:00:52.043688Z",
          "shell.execute_reply.started": "2022-03-05T17:00:52.031363Z",
          "shell.execute_reply": "2022-03-05T17:00:52.042812Z"
        },
        "trusted": true,
        "id": "3Gz1hKsEQLkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying the LGBM model "
      ],
      "metadata": {
        "id": "Jec3GMpbQLki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1, metric_over_time1, mse1, rmse1, pcc1, execution_time1 = LBGM_model(X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:00:52.045114Z",
          "iopub.execute_input": "2022-03-05T17:00:52.045553Z",
          "iopub.status.idle": "2022-03-05T17:06:16.333876Z",
          "shell.execute_reply.started": "2022-03-05T17:00:52.04551Z",
          "shell.execute_reply": "2022-03-05T17:06:16.333078Z"
        },
        "trusted": true,
        "id": "QeD8HTqiQLki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the graph above, we can see how the metric improves as the training goes on. The mean squared error (MSE), also referred to as square loss or l2, is decreasing steadily."
      ],
      "metadata": {
        "id": "4lZuHKCVQLki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightGBM has a nice build in function for plotting the feature importance. Feature importance can be displayed as \"gain\", showing the total gains of splits which use the feature, or \"split\", showing the numbers of times the feature is used in a model."
      ],
      "metadata": {
        "id": "OM7wjhIvQLki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at which features lgbm deems important\n",
        "# https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_importance.html\n",
        "lgb.plot_importance(model1, figsize=(20,40), importance_type='gain', max_num_features=300) # importance_type: gain/split: V7 has 'split'\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:06:16.335654Z",
          "iopub.execute_input": "2022-03-05T17:06:16.335952Z",
          "iopub.status.idle": "2022-03-05T17:06:20.471604Z",
          "shell.execute_reply.started": "2022-03-05T17:06:16.33591Z",
          "shell.execute_reply": "2022-03-05T17:06:20.470932Z"
        },
        "trusted": true,
        "id": "6RiDmZpUQLkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting rid of features with little importance\n",
        "imp = pd.DataFrame({'Value':model1.feature_importance(importance_type='gain'),'Feature':X_train.columns}).sort_values(by=\"Value\",ascending=False).reset_index(drop=True)\n",
        "\n",
        "#imp.Value.value_counts()\n",
        "imp = imp[imp.Value>100]  # remove all features with gain lower than 100\n",
        "new_feature_list = list(imp.Feature)\n",
        "print(\"Number of features, new: \", len(new_feature_list))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:06:20.473149Z",
          "iopub.execute_input": "2022-03-05T17:06:20.473603Z",
          "iopub.status.idle": "2022-03-05T17:06:20.48427Z",
          "shell.execute_reply.started": "2022-03-05T17:06:20.473564Z",
          "shell.execute_reply": "2022-03-05T17:06:20.483554Z"
        },
        "trusted": true,
        "id": "VEvkIKGXQLkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model1 to disk\n",
        "filename = 'lgbm_model_pre_feat_selection.sav'\n",
        "pickle.dump(model1, open(filename, 'wb'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:06:20.485625Z",
          "iopub.execute_input": "2022-03-05T17:06:20.486107Z",
          "iopub.status.idle": "2022-03-05T17:06:20.516558Z",
          "shell.execute_reply.started": "2022-03-05T17:06:20.48606Z",
          "shell.execute_reply": "2022-03-05T17:06:20.515921Z"
        },
        "trusted": true,
        "id": "612q8tGFQLkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model1, imp # free up memory"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:06:20.520616Z",
          "iopub.execute_input": "2022-03-05T17:06:20.5223Z",
          "iopub.status.idle": "2022-03-05T17:06:20.537129Z",
          "shell.execute_reply.started": "2022-03-05T17:06:20.52226Z",
          "shell.execute_reply": "2022-03-05T17:06:20.536306Z"
        },
        "trusted": true,
        "id": "1OdTFxYuQLkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-applying LGBM after feature selection"
      ],
      "metadata": {
        "id": "H71U1O3yQLkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2, metric_over_time2, mse2, rmse2, pcc2, execution_time2 = LBGM_model(X_train[new_feature_list], y_train, X_val[new_feature_list], y_val)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:06:20.540755Z",
          "iopub.execute_input": "2022-03-05T17:06:20.541362Z",
          "iopub.status.idle": "2022-03-05T17:09:47.067898Z",
          "shell.execute_reply.started": "2022-03-05T17:06:20.541328Z",
          "shell.execute_reply": "2022-03-05T17:09:47.067052Z"
        },
        "trusted": true,
        "id": "U3sr1pXdQLkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the change in evaluation metrics between both models, pre- vs post- feature selection"
      ],
      "metadata": {
        "id": "GtwgCIFgQLkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_percentage_change(current_param, previous_param):\n",
        "    \"\"\"Return the percentage change of a parameter with respect to initial parameter value\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    current_param : float/int\n",
        "        Current parameter to compare.\n",
        "    previosu_param, : float/int\n",
        "        Previous version of parameter to compare.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    %_change : float/int\n",
        "        The percentage change of both parameters w.r.t. previous state.\n",
        "    \"\"\"\n",
        "    if current_param == previous_param:\n",
        "        return 0\n",
        "    try:\n",
        "        return round(((current_param - previous_param) / previous_param) * 100.0, 5)\n",
        "    except ZeroDivisionError:\n",
        "        return float('inf')\n",
        "\n",
        "    "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:09:47.06964Z",
          "iopub.execute_input": "2022-03-05T17:09:47.07002Z",
          "iopub.status.idle": "2022-03-05T17:09:47.076448Z",
          "shell.execute_reply.started": "2022-03-05T17:09:47.069956Z",
          "shell.execute_reply": "2022-03-05T17:09:47.075236Z"
        },
        "trusted": true,
        "id": "uw_V4i66QLkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Change in MSE: {} %\".format(get_percentage_change(mse2, mse1)))\n",
        "print(\"Change in RMSE: {} %\".format(get_percentage_change(rmse2, rmse1)))\n",
        "print(\"Change in PCC: {} %\".format(get_percentage_change(pcc2, pcc1)))\n",
        "print(\"Change in execution time: {} %\".format(get_percentage_change(execution_time2, execution_time1)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:09:47.078269Z",
          "iopub.execute_input": "2022-03-05T17:09:47.078667Z",
          "iopub.status.idle": "2022-03-05T17:09:47.089044Z",
          "shell.execute_reply.started": "2022-03-05T17:09:47.078624Z",
          "shell.execute_reply": "2022-03-05T17:09:47.087861Z"
        },
        "trusted": true,
        "id": "4lvq1RcvQLkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that there is a small decrease in the MSE and RMSE, but an increase in the PCC, when the model is only applied using the features selected. So while MSE, RSME and PCC are comparable between LightGBM  using all 300 features and LightGBM using only the more important features, the training time is greatly reduced! "
      ],
      "metadata": {
        "id": "khMn_f1bQLkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model2 to disk\n",
        "filename = 'lgbm_model_post_feat_selection.sav'\n",
        "pickle.dump(model2, open(filename, 'wb'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:09:47.090686Z",
          "iopub.execute_input": "2022-03-05T17:09:47.090994Z",
          "iopub.status.idle": "2022-03-05T17:09:47.111887Z",
          "shell.execute_reply.started": "2022-03-05T17:09:47.090953Z",
          "shell.execute_reply": "2022-03-05T17:09:47.111263Z"
        },
        "trusted": true,
        "id": "XZRb7Qo5QLkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:19.044216Z",
          "iopub.execute_input": "2022-03-05T17:11:19.045071Z",
          "iopub.status.idle": "2022-03-05T17:11:19.049575Z",
          "shell.execute_reply.started": "2022-03-05T17:11:19.045018Z",
          "shell.execute_reply": "2022-03-05T17:11:19.048763Z"
        },
        "trusted": true,
        "id": "AXvAFrizQLkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random forest for feature selection"
      ],
      "metadata": {
        "id": "b6woSS-6QLkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import time\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import shap\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "import random\n",
        "import os\n",
        "import gc\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "#from sklearn.linear_model import LinearRegression\n",
        "#print(\"scikit-learn version: {}\". format(sklearn.__version__))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:22.507771Z",
          "iopub.execute_input": "2022-03-05T17:11:22.508498Z",
          "iopub.status.idle": "2022-03-05T17:11:23.883009Z",
          "shell.execute_reply.started": "2022-03-05T17:11:22.508448Z",
          "shell.execute_reply": "2022-03-05T17:11:23.88191Z"
        },
        "trusted": true,
        "id": "9sJAdiPnQLkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import competition data\n",
        "\n",
        "train = pd.read_csv('../input/ubiquant-market-prediction/train.csv', nrows=10000)\n",
        "example_test = pd.read_csv('../input/ubiquant-market-prediction/example_test.csv')\n",
        "example_sample_submission =  pd.read_csv('../input/ubiquant-market-prediction/example_sample_submission.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:23.885066Z",
          "iopub.execute_input": "2022-03-05T17:11:23.885474Z",
          "iopub.status.idle": "2022-03-05T17:11:24.622968Z",
          "shell.execute_reply.started": "2022-03-05T17:11:23.885419Z",
          "shell.execute_reply": "2022-03-05T17:11:24.622174Z"
        },
        "trusted": true,
        "id": "Tp9Zw9ECQLkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling\n",
        "https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "\n",
        "Note : 6.3.1.3. Scaling data with outliers If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use RobustScaler as a drop-in replacement instead. It uses more robust estimates for the center and range of your data."
      ],
      "metadata": {
        "id": "kDmGW5eWQLkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = train\n",
        "scaler = StandardScaler()\n",
        "X = np.array(df.drop(['row_id', 'time_id', 'investment_id', 'target'], axis = 1))\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "y = np.array(df['target'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:24.626287Z",
          "iopub.execute_input": "2022-03-05T17:11:24.626493Z",
          "iopub.status.idle": "2022-03-05T17:11:24.675516Z",
          "shell.execute_reply.started": "2022-03-05T17:11:24.626466Z",
          "shell.execute_reply": "2022-03-05T17:11:24.674704Z"
        },
        "trusted": true,
        "id": "CnW6Oo3yQLkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_mem_usage(df):\n",
        "  \n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    \n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "     \n",
        "    return df\n",
        "\n",
        "df = reduce_mem_usage(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:24.677111Z",
          "iopub.execute_input": "2022-03-05T17:11:24.677424Z",
          "iopub.status.idle": "2022-03-05T17:11:25.454275Z",
          "shell.execute_reply.started": "2022-03-05T17:11:24.677379Z",
          "shell.execute_reply": "2022-03-05T17:11:25.453458Z"
        },
        "trusted": true,
        "id": "8SCFUgNbQLkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [f'f_{i}' for i in range(300)]\n",
        "#print(features)\n",
        "df_median = df[features].head(30000).median()\n",
        "#print(df_median)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:25.457147Z",
          "iopub.execute_input": "2022-03-05T17:11:25.457661Z",
          "iopub.status.idle": "2022-03-05T17:11:25.569227Z",
          "shell.execute_reply.started": "2022-03-05T17:11:25.457612Z",
          "shell.execute_reply": "2022-03-05T17:11:25.568378Z"
        },
        "trusted": true,
        "id": "W8FpepqkQLkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RandomForestRegressor\n"
      ],
      "metadata": {
        "id": "oAFDl-SnQLkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:25.570719Z",
          "iopub.execute_input": "2022-03-05T17:11:25.571015Z",
          "iopub.status.idle": "2022-03-05T17:11:25.663209Z",
          "shell.execute_reply.started": "2022-03-05T17:11:25.570963Z",
          "shell.execute_reply": "2022-03-05T17:11:25.662371Z"
        },
        "trusted": true,
        "id": "FFf48obUQLkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timeseries split from sklearn - work in progress"
      ],
      "metadata": {
        "id": "mX69fs7cQLkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "ts_cv = TimeSeriesSplit(\n",
        "    n_splits=5,\n",
        "    #gap=48,\n",
        "    max_train_size=10000,\n",
        "    #test_size=1000,\n",
        ")\n",
        "x=pd.DataFrame(X)\n",
        "all_splits = list(ts_cv.split(X, y))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:25.664601Z",
          "iopub.execute_input": "2022-03-05T17:11:25.665044Z",
          "iopub.status.idle": "2022-03-05T17:11:25.672644Z",
          "shell.execute_reply.started": "2022-03-05T17:11:25.664988Z",
          "shell.execute_reply": "2022-03-05T17:11:25.671211Z"
        },
        "trusted": true,
        "id": "mUejbRoJQLkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "View the data in splits"
      ],
      "metadata": {
        "id": "5INxyqEPQLkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_3, test_3 = all_splits[3]\n",
        "#x.iloc[test_3]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:25.674298Z",
          "iopub.execute_input": "2022-03-05T17:11:25.674557Z",
          "iopub.status.idle": "2022-03-05T17:11:25.682893Z",
          "shell.execute_reply.started": "2022-03-05T17:11:25.674513Z",
          "shell.execute_reply": "2022-03-05T17:11:25.682065Z"
        },
        "trusted": true,
        "id": "T5kpprEQQLkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_4, test_4 = all_splits[4]\n",
        "#x.iloc[test_4]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:25.684585Z",
          "iopub.execute_input": "2022-03-05T17:11:25.686315Z",
          "iopub.status.idle": "2022-03-05T17:11:25.691181Z",
          "shell.execute_reply.started": "2022-03-05T17:11:25.68628Z",
          "shell.execute_reply": "2022-03-05T17:11:25.690192Z"
        },
        "trusted": true,
        "id": "65JTfpFLQLkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean decrease in impurity - gini importance\n",
        "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
        "\n",
        "Feature importances are provided by the fitted attribute featureimportances and they are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree."
      ],
      "metadata": {
        "id": "U7KCCsWRQLko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(X_train, y_train)\n",
        "rf_confidence = rf.score(X_test, y_test)\n",
        "rf_confidence"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:11:25.69266Z",
          "iopub.execute_input": "2022-03-05T17:11:25.693101Z",
          "iopub.status.idle": "2022-03-05T17:14:32.156649Z",
          "shell.execute_reply.started": "2022-03-05T17:11:25.69306Z",
          "shell.execute_reply": "2022-03-05T17:14:32.155897Z"
        },
        "trusted": true,
        "id": "D2g65TU6QLko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "importances = rf.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:14:32.158109Z",
          "iopub.execute_input": "2022-03-05T17:14:32.158928Z",
          "iopub.status.idle": "2022-03-05T17:14:32.203649Z",
          "shell.execute_reply.started": "2022-03-05T17:14:32.158887Z",
          "shell.execute_reply": "2022-03-05T17:14:32.202783Z"
        },
        "trusted": true,
        "id": "Qj1-Ik_EQLko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = [f\"feature {i}\" for i in range(X.shape[1])]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:14:32.2054Z",
          "iopub.execute_input": "2022-03-05T17:14:32.205664Z",
          "iopub.status.idle": "2022-03-05T17:14:32.209908Z",
          "shell.execute_reply.started": "2022-03-05T17:14:32.205627Z",
          "shell.execute_reply": "2022-03-05T17:14:32.209205Z"
        },
        "trusted": true,
        "id": "k9ynXfy-QLko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_importances = pd.Series(importances, index=features)\n",
        "#forest_importances.sort_values(ascending=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:14:32.211394Z",
          "iopub.execute_input": "2022-03-05T17:14:32.211899Z",
          "iopub.status.idle": "2022-03-05T17:14:32.219873Z",
          "shell.execute_reply.started": "2022-03-05T17:14:32.211858Z",
          "shell.execute_reply": "2022-03-05T17:14:32.219162Z"
        },
        "trusted": true,
        "id": "1tUIMUHLQLkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,100))\n",
        "forest_importances.sort_values(ascending=True).plot(kind=\"barh\", fontsize=12)\n",
        "plt.ylabel('MDI')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:14:32.22446Z",
          "iopub.execute_input": "2022-03-05T17:14:32.226225Z",
          "iopub.status.idle": "2022-03-05T17:14:35.678125Z",
          "shell.execute_reply.started": "2022-03-05T17:14:32.226194Z",
          "shell.execute_reply": "2022-03-05T17:14:35.677269Z"
        },
        "trusted": true,
        "id": "lZ2Wye0rQLkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Permutation importances\n",
        "Permutation feature importance overcomes limitations of the impurity-based feature importance: they do not have a bias toward high-cardinality features and can be computed on a left-out test set.\n"
      ],
      "metadata": {
        "id": "WUKVw6gKQLkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "#imp = permutation_importance(rf, X_train, y_train, oob_regression_r2_score)\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "result = permutation_importance(\n",
        "    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
        ")\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
        "\n",
        "forest_importances = pd.Series(result.importances_mean, index=features)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:14:35.679565Z",
          "iopub.execute_input": "2022-03-05T17:14:35.679896Z",
          "iopub.status.idle": "2022-03-05T17:18:14.020462Z",
          "shell.execute_reply.started": "2022-03-05T17:14:35.679858Z",
          "shell.execute_reply": "2022-03-05T17:18:14.019497Z"
        },
        "trusted": true,
        "id": "rDqdiHI1QLkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,100))\n",
        "forest_importances.sort_values(ascending=True).plot(kind=\"barh\", fontsize=12)\n",
        "plt.ylabel('permutation')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:18:14.02264Z",
          "iopub.execute_input": "2022-03-05T17:18:14.023174Z",
          "iopub.status.idle": "2022-03-05T17:18:17.493726Z",
          "shell.execute_reply.started": "2022-03-05T17:18:14.023123Z",
          "shell.execute_reply": "2022-03-05T17:18:17.492898Z"
        },
        "trusted": true,
        "id": "Bb2xWFcRQLkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost for feature selection"
      ],
      "metadata": {
        "id": "NW3sA1UaQLkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the librairies"
      ],
      "metadata": {
        "id": "aIqz1cqaQLkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from scipy import stats\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import *\n",
        "import warnings\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "#import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:18:17.49547Z",
          "iopub.execute_input": "2022-03-05T17:18:17.495982Z",
          "iopub.status.idle": "2022-03-05T17:18:22.464444Z",
          "shell.execute_reply.started": "2022-03-05T17:18:17.495942Z",
          "shell.execute_reply": "2022-03-05T17:18:22.463644Z"
        },
        "trusted": true,
        "id": "yRhh0OdVQLkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset."
      ],
      "metadata": {
        "id": "8DkIZvEEQLkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "n_features = 300\n",
        "features = [f'f_{i}' for i in range(n_features)]\n",
        "train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n",
        "train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:18:22.466183Z",
          "iopub.execute_input": "2022-03-05T17:18:22.466466Z",
          "iopub.status.idle": "2022-03-05T17:18:23.936712Z",
          "shell.execute_reply.started": "2022-03-05T17:18:22.466425Z",
          "shell.execute_reply": "2022-03-05T17:18:23.935947Z"
        },
        "trusted": true,
        "id": "1OuAI3KgQLkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Taking the list of features "
      ],
      "metadata": {
        "id": "vvlojIFAQLkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L = [ x for x in train.columns if x != 'target' ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:18:23.93813Z",
          "iopub.execute_input": "2022-03-05T17:18:23.938972Z",
          "iopub.status.idle": "2022-03-05T17:18:23.943858Z",
          "shell.execute_reply.started": "2022-03-05T17:18:23.938927Z",
          "shell.execute_reply": "2022-03-05T17:18:23.943152Z"
        },
        "trusted": true,
        "id": "fTGmnm_IQLkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### splitting the dataset into test and train"
      ],
      "metadata": {
        "id": "vOEpOZ8_QLku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split((train.head(300000))[L], (train.head(300000))['target'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:18:23.945179Z",
          "iopub.execute_input": "2022-03-05T17:18:23.945819Z",
          "iopub.status.idle": "2022-03-05T17:18:25.550443Z",
          "shell.execute_reply.started": "2022-03-05T17:18:23.945771Z",
          "shell.execute_reply": "2022-03-05T17:18:25.549633Z"
        },
        "trusted": true,
        "id": "cP2bZeSMQLku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating DMatrix \n",
        "\n",
        "We need to put our dataset in the format XGBoost works with."
      ],
      "metadata": {
        "id": "wxqQYEpDQLkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:18:25.551861Z",
          "iopub.execute_input": "2022-03-05T17:18:25.552143Z",
          "iopub.status.idle": "2022-03-05T17:18:28.238665Z",
          "shell.execute_reply.started": "2022-03-05T17:18:25.552104Z",
          "shell.execute_reply": "2022-03-05T17:18:28.238061Z"
        },
        "trusted": true,
        "id": "NrBFlH1wQLkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting the parametters of the model"
      ],
      "metadata": {
        "id": "WV4mJ3kDQLkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        " 'booster': 'gbtree',\n",
        "    'max_depth': 5, \n",
        "    'learning_rate': 0.1,\n",
        "    'sample_type': 'uniform',\n",
        "    'normalize_type': 'tree',\n",
        "    'objective': 'binary:hinge',\n",
        "    'rate_drop': 0.1,\n",
        "    'n_estimators': 500,\n",
        "    'silent' :1\n",
        "}\n",
        "\n",
        "num_rounds = 20"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:18:28.240063Z",
          "iopub.execute_input": "2022-03-05T17:18:28.240572Z",
          "iopub.status.idle": "2022-03-05T17:18:28.24647Z",
          "shell.execute_reply.started": "2022-03-05T17:18:28.240532Z",
          "shell.execute_reply": "2022-03-05T17:18:28.24579Z"
        },
        "trusted": true,
        "id": "mXKO0DAzQLkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Watchlist\n",
        "\n",
        "Watchlist is a list of xgb.DMatrix, each of them is tagged with name.\n",
        "Watchlist allows us to monitor the evaluation result on all data in the list"
      ],
      "metadata": {
        "id": "sPbP0zggQLkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "watchlist = [(dtest, 'test'), (dtrain,'train')]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:18:28.249573Z",
          "iopub.execute_input": "2022-03-05T17:18:28.251975Z",
          "iopub.status.idle": "2022-03-05T17:18:28.257474Z",
          "shell.execute_reply.started": "2022-03-05T17:18:28.251939Z",
          "shell.execute_reply": "2022-03-05T17:18:28.256776Z"
        },
        "trusted": true,
        "id": "pqIbO2PCQLkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model."
      ],
      "metadata": {
        "id": "JHpL2FF4QLkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:18:28.258738Z",
          "iopub.execute_input": "2022-03-05T17:18:28.259594Z",
          "iopub.status.idle": "2022-03-05T17:20:14.851033Z",
          "shell.execute_reply.started": "2022-03-05T17:18:28.259554Z",
          "shell.execute_reply": "2022-03-05T17:20:14.850198Z"
        },
        "trusted": true,
        "id": "51qKwaczQLkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dumping the model to a txt file to use later."
      ],
      "metadata": {
        "id": "MKgADttaQLkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to see how the model looks\n",
        "#bst.dump_model('dump.raw.txt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:20:14.852228Z",
          "iopub.execute_input": "2022-03-05T17:20:14.852479Z",
          "iopub.status.idle": "2022-03-05T17:20:14.856933Z",
          "shell.execute_reply.started": "2022-03-05T17:20:14.852443Z",
          "shell.execute_reply": "2022-03-05T17:20:14.855862Z"
        },
        "trusted": true,
        "id": "hR6lZOliQLkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the Fscore of each feature"
      ],
      "metadata": {
        "id": "vXW9dU33QLkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb.plot_importance(bst)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:20:14.858003Z",
          "iopub.execute_input": "2022-03-05T17:20:14.858362Z",
          "iopub.status.idle": "2022-03-05T17:20:18.34024Z",
          "shell.execute_reply.started": "2022-03-05T17:20:14.858322Z",
          "shell.execute_reply": "2022-03-05T17:20:18.339402Z"
        },
        "trusted": true,
        "id": "WJsFclP0QLkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List of feature"
      ],
      "metadata": {
        "id": "faZcOKECQLkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L = bst.get_score(importance_type='gain')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:20:18.34162Z",
          "iopub.execute_input": "2022-03-05T17:20:18.342019Z",
          "iopub.status.idle": "2022-03-05T17:20:18.350123Z",
          "shell.execute_reply.started": "2022-03-05T17:20:18.34196Z",
          "shell.execute_reply": "2022-03-05T17:20:18.349329Z"
        },
        "trusted": true,
        "id": "5URj_Mq4QLkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sort the list of feature in decreasing order."
      ],
      "metadata": {
        "id": "UtJO5eR6QLkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sortedfeatures = dict(sorted(L.items(), key=lambda item: item[1]),reverse=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:20:18.351554Z",
          "iopub.execute_input": "2022-03-05T17:20:18.352037Z",
          "iopub.status.idle": "2022-03-05T17:20:18.35793Z",
          "shell.execute_reply.started": "2022-03-05T17:20:18.351975Z",
          "shell.execute_reply": "2022-03-05T17:20:18.357142Z"
        },
        "trusted": true,
        "id": "kJMGe6qzQLkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sorted features in order of importance. Showing first 5 of the dictionary."
      ],
      "metadata": {
        "id": "QGKBVDFRQLkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(sortedfeatures.items())[:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:20:18.359491Z",
          "iopub.execute_input": "2022-03-05T17:20:18.359994Z",
          "iopub.status.idle": "2022-03-05T17:20:18.369976Z",
          "shell.execute_reply.started": "2022-03-05T17:20:18.359953Z",
          "shell.execute_reply": "2022-03-05T17:20:18.369264Z"
        },
        "trusted": true,
        "id": "onI2LRLIQLkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAPIDS Random Forest\n",
        "Using cudf and cuml librairies "
      ],
      "metadata": {
        "id": "3FiWEuktQLky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cudf\n",
        "\n",
        "df = cudf.read_parquet(\"../input/ubiquant-parquet/train_low_mem.parquet\")\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:20:18.372392Z",
          "iopub.execute_input": "2022-03-05T17:20:18.373248Z",
          "iopub.status.idle": "2022-03-05T17:21:07.784661Z",
          "shell.execute_reply.started": "2022-03-05T17:20:18.373206Z",
          "shell.execute_reply": "2022-03-05T17:21:07.783792Z"
        },
        "trusted": true,
        "id": "JWWlvXJ5QLky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"time_id\"].max(), df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:21:07.786231Z",
          "iopub.execute_input": "2022-03-05T17:21:07.786801Z",
          "iopub.status.idle": "2022-03-05T17:21:07.803915Z",
          "shell.execute_reply.started": "2022-03-05T17:21:07.786756Z",
          "shell.execute_reply": "2022-03-05T17:21:07.803063Z"
        },
        "trusted": true,
        "id": "ePo_DTU0QLky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DROP_BEFORE = 600\n",
        "\n",
        "df = df[df[\"time_id\"] > DROP_BEFORE].reset_index(drop=True)\n",
        "df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:21:07.805452Z",
          "iopub.execute_input": "2022-03-05T17:21:07.805823Z",
          "iopub.status.idle": "2022-03-05T17:21:08.105637Z",
          "shell.execute_reply.started": "2022-03-05T17:21:07.80578Z",
          "shell.execute_reply": "2022-03-05T17:21:08.104908Z"
        },
        "trusted": true,
        "id": "MFFXFE8XQLkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DROP_AFTER = 1000\n",
        "\n",
        "df = df[df[\"time_id\"] < DROP_AFTER].reset_index(drop=True)\n",
        "df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:21:08.106829Z",
          "iopub.execute_input": "2022-03-05T17:21:08.107431Z",
          "iopub.status.idle": "2022-03-05T17:21:08.470814Z",
          "shell.execute_reply.started": "2022-03-05T17:21:08.107386Z",
          "shell.execute_reply": "2022-03-05T17:21:08.469904Z"
        },
        "trusted": true,
        "id": "yyKNDjq0QLkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy\n",
        "import cuml\n",
        "\n",
        "print(\"cuML version:\", cuml.__version__)\n",
        "\n",
        "WINDOW = 20\n",
        "START = 700\n",
        "N_SPLITS = 15\n",
        "\n",
        "cv = []\n",
        "\n",
        "for i in range(N_SPLITS):\n",
        "    train_ind = cupy.where(df[\"time_id\"].values <= START + i*WINDOW)[0]\n",
        "    val_ind = cupy.where((df[\"time_id\"].values > START + i*WINDOW) & (df[\"time_id\"].values <= START + (i+1)*WINDOW))[0]\n",
        "    cv.append((cupy.asnumpy(train_ind), cupy.asnumpy(val_ind)))\n",
        "    print(len(train_ind), len(val_ind))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:21:08.472424Z",
          "iopub.execute_input": "2022-03-05T17:21:08.472903Z",
          "iopub.status.idle": "2022-03-05T17:21:12.874415Z",
          "shell.execute_reply.started": "2022-03-05T17:21:08.472861Z",
          "shell.execute_reply": "2022-03-05T17:21:12.873559Z"
        },
        "trusted": true,
        "id": "cRym02U0QLkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [col for col in df.columns if col not in {\"row_id\", \"target\", \"investment_id\", \"time_id\"}]\n",
        "features += [\"investment_te\"]\n",
        "len(features)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:21:12.875881Z",
          "iopub.execute_input": "2022-03-05T17:21:12.876205Z",
          "iopub.status.idle": "2022-03-05T17:21:12.883392Z",
          "shell.execute_reply.started": "2022-03-05T17:21:12.876164Z",
          "shell.execute_reply": "2022-03-05T17:21:12.882649Z"
        },
        "trusted": true,
        "id": "rTG83LPTQLkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAPIDSModel:\n",
        "    def __init__(self):\n",
        "        self.te = cuml.preprocessing.TargetEncoder()\n",
        "        self.rf = cuml.ensemble.RandomForestRegressor(n_estimators=256, split_criterion=\"mse\", bootstrap=True,\n",
        "                                                      max_samples=0.6, min_samples_leaf=64, max_features=0.6, n_bins=512)\n",
        "        \n",
        "    def calculate_sample_weight(self, train_df):\n",
        "        time_mean = train_df.groupby(\"time_id\")[\"target\"].mean().reset_index().rename(columns={\"target\": \"target_mean\"})\n",
        "        time_std = train_df.groupby(\"time_id\")[\"target\"].std().reset_index().rename(columns={\"target\": \"target_std\"})\n",
        "\n",
        "        train_df = train_df.merge(time_mean, on=\"time_id\", how=\"left\").merge(time_std, on=\"time_id\", how=\"left\")\n",
        "        train_df = train_df.sort_values([\"time_id\", \"investment_id\"]).reset_index(drop=True)\n",
        "\n",
        "        train_df[\"norm_target\"] = (train_df[\"target\"] - train_df[\"target_mean\"])/train_df[\"target_std\"]\n",
        "        train_df[\"sw\"] = (train_df[\"norm_target\"].abs() + 1)/2\n",
        "                \n",
        "        return train_df\n",
        "        \n",
        "    def fit(self, train_df):\n",
        "        train_df[\"investment_te\"] = self.te.fit_transform(train_df[\"investment_id\"], train_df[\"target\"]).astype(\"float32\")\n",
        "        #train_df = self.calculate_sample_weight(train_df)\n",
        "        \n",
        "        #self.svr.fit(train_df[features], train_df[\"target\"], sample_weight=train_df[\"sw\"])\n",
        "        self.rf.fit(train_df[features], train_df[\"target\"])\n",
        "\n",
        "        return self\n",
        "        \n",
        "    def predict(self, test_df):\n",
        "        test_df[\"investment_te\"] = self.te.transform(test_df[\"investment_id\"]).astype(\"float32\").get()\n",
        "        #return 0.7*self.rf.predict(test_df[features]) + 0.3*self.svr.predict(test_df[features])\n",
        "        return self.rf.predict(test_df[features])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:21:12.884843Z",
          "iopub.execute_input": "2022-03-05T17:21:12.885282Z",
          "iopub.status.idle": "2022-03-05T17:21:12.899776Z",
          "shell.execute_reply.started": "2022-03-05T17:21:12.885239Z",
          "shell.execute_reply": "2022-03-05T17:21:12.898919Z"
        },
        "trusted": true,
        "id": "0ljB44r9QLk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def evaluate(val_df):\n",
        "    scores = []\n",
        "    for time_id in val_df[\"time_id\"].unique().values_host:\n",
        "        time_df = val_df[val_df[\"time_id\"] == time_id]\n",
        "        scores.append(time_df[\"target\"].corr(time_df[\"pred\"]))\n",
        "\n",
        "    return cupy.mean(cupy.array(scores))\n",
        "\n",
        "\n",
        "val_scores = []\n",
        "\n",
        "\n",
        "for f, (train_ind, val_ind) in tqdm(enumerate(cv), total=len(cv)):\n",
        "    train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n",
        "\n",
        "    model = RAPIDSModel().fit(train_df)\n",
        "    y_pred = model.predict(val_df)\n",
        "    val_df[\"pred\"] = y_pred.values\n",
        "    \n",
        "    val_scores.append(evaluate(val_df).item())\n",
        "    \n",
        "val_scores = cupy.array(val_scores)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:21:12.903356Z",
          "iopub.execute_input": "2022-03-05T17:21:12.903614Z",
          "iopub.status.idle": "2022-03-05T17:55:23.11785Z",
          "shell.execute_reply.started": "2022-03-05T17:21:12.903576Z",
          "shell.execute_reply": "2022-03-05T17:55:23.117076Z"
        },
        "trusted": true,
        "id": "2d7mIEbEQLk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Validation scores:\", val_scores)\n",
        "print(\"Mean:\", cupy.mean(val_scores))\n",
        "print(\"STD:\", cupy.std(val_scores))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:55:23.119964Z",
          "iopub.execute_input": "2022-03-05T17:55:23.120495Z",
          "iopub.status.idle": "2022-03-05T17:55:23.907894Z",
          "shell.execute_reply.started": "2022-03-05T17:55:23.120451Z",
          "shell.execute_reply": "2022-03-05T17:55:23.907121Z"
        },
        "trusted": true,
        "id": "M7aJpa_KQLk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RAPIDSModel().fit(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:55:23.909433Z",
          "iopub.execute_input": "2022-03-05T17:55:23.909749Z",
          "iopub.status.idle": "2022-03-05T17:59:19.233721Z",
          "shell.execute_reply.started": "2022-03-05T17:55:23.909703Z",
          "shell.execute_reply": "2022-03-05T17:59:19.232888Z"
        },
        "trusted": true,
        "id": "_3EPlBp1QLk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pickling random forest model\n",
        "https://docs.rapids.ai/api/cuml/nightly/pickling_cuml_models.html"
      ],
      "metadata": {
        "id": "FpxmNaB5QLk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(model, open(\"ubiquant_cuml_rf_model.pkl\", \"wb\"))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:59:19.235598Z",
          "iopub.execute_input": "2022-03-05T17:59:19.235861Z",
          "iopub.status.idle": "2022-03-05T17:59:19.547706Z",
          "shell.execute_reply.started": "2022-03-05T17:59:19.235824Z",
          "shell.execute_reply": "2022-03-05T17:59:19.546917Z"
        },
        "trusted": true,
        "id": "aEXCxZWRQLk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:59:19.553893Z",
          "iopub.execute_input": "2022-03-05T17:59:19.554369Z",
          "iopub.status.idle": "2022-03-05T17:59:19.559882Z",
          "shell.execute_reply.started": "2022-03-05T17:59:19.554329Z",
          "shell.execute_reply": "2022-03-05T17:59:19.558854Z"
        },
        "trusted": true,
        "id": "9vmyGTJVQLk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "n_features = 300\n",
        "features = [f'f_{i}' for i in range(n_features)]\n",
        "feature_columns = ['investment_id', 'time_id'] + features\n",
        "train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n",
        "train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:59:19.561653Z",
          "iopub.execute_input": "2022-03-05T17:59:19.561927Z",
          "iopub.status.idle": "2022-03-05T17:59:21.435544Z",
          "shell.execute_reply.started": "2022-03-05T17:59:19.561888Z",
          "shell.execute_reply": "2022-03-05T17:59:21.434826Z"
        },
        "trusted": true,
        "id": "RX6ylLeUQLk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "investment_id = train.pop(\"investment_id\")\n",
        "investment_id.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:59:21.437145Z",
          "iopub.execute_input": "2022-03-05T17:59:21.437647Z",
          "iopub.status.idle": "2022-03-05T17:59:21.452688Z",
          "shell.execute_reply.started": "2022-03-05T17:59:21.4376Z",
          "shell.execute_reply": "2022-03-05T17:59:21.451861Z"
        },
        "trusted": true,
        "id": "tmUdi9dWQLk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = train.pop(\"time_id\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:59:21.454Z",
          "iopub.execute_input": "2022-03-05T17:59:21.454386Z",
          "iopub.status.idle": "2022-03-05T17:59:21.466535Z",
          "shell.execute_reply.started": "2022-03-05T17:59:21.454342Z",
          "shell.execute_reply": "2022-03-05T17:59:21.465547Z"
        },
        "trusted": true,
        "id": "6-gxRxa2QLk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = train.pop(\"target\")\n",
        "y.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:59:21.467944Z",
          "iopub.execute_input": "2022-03-05T17:59:21.468753Z",
          "iopub.status.idle": "2022-03-05T17:59:21.484767Z",
          "shell.execute_reply.started": "2022-03-05T17:59:21.468713Z",
          "shell.execute_reply": "2022-03-05T17:59:21.483889Z"
        },
        "trusted": true,
        "id": "WY98WpcrQLk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a IntegerLookup layer for investment_id input"
      ],
      "metadata": {
        "id": "wAS1L_XaQLk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "investment_ids = list(investment_id.unique())\n",
        "investment_id_size = len(investment_ids) + 1\n",
        "investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n",
        "with tf.device(\"cpu\"):\n",
        "    investment_id_lookup_layer.adapt(investment_id)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T17:59:21.486103Z",
          "iopub.execute_input": "2022-03-05T17:59:21.486461Z",
          "iopub.status.idle": "2022-03-05T18:00:55.571146Z",
          "shell.execute_reply.started": "2022-03-05T17:59:21.486392Z",
          "shell.execute_reply": "2022-03-05T18:00:55.570175Z"
        },
        "trusted": true,
        "id": "NnfWIDUEQLk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Tensorflow dataset"
      ],
      "metadata": {
        "id": "RY9P0_2fQLk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def preprocess(X, y):\n",
        "    print(X)\n",
        "    print(y)\n",
        "    return X, y\n",
        "def make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n",
        "    ds = ds.map(preprocess)\n",
        "    if mode == \"train\":\n",
        "        ds = ds.shuffle(256)\n",
        "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return ds"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T18:00:55.572863Z",
          "iopub.execute_input": "2022-03-05T18:00:55.573758Z",
          "iopub.status.idle": "2022-03-05T18:00:55.580447Z",
          "shell.execute_reply.started": "2022-03-05T18:00:55.573698Z",
          "shell.execute_reply": "2022-03-05T18:00:55.579592Z"
        },
        "trusted": true,
        "id": "DvbL3kLpQLk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "eVmHsYvwQLk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creaing dnn models to be used for the ensemble modelling. The swish activation function as it outperforms ReLu with diverse size of batches\n"
      ],
      "metadata": {
        "id": "K3CWwjtWQLk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
        "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
        "    \n",
        "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
        "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
        "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
        "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
        "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
        "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
        "    \n",
        "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
        "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
        "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
        "    \n",
        "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
        "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
        "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
        "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
        "    return model\n",
        "def get_model2():\n",
        "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
        "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
        "    \n",
        "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
        "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
        "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
        "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n",
        "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
        "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
        "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
        "   # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n",
        "   \n",
        "    \n",
        "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
        "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
        "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
        "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
        "    feature_x = layers.Dropout(0.65)(feature_x)\n",
        "    \n",
        "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
        "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "   # x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "  #  x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    x = layers.Dropout(0.75)(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
        "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
        "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
        "    return model\n",
        "def get_model3():\n",
        "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
        "    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n",
        "    \n",
        "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
        "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
        "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
        "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
        "    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n",
        "    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n",
        "    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n",
        "    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
        "    \n",
        "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
        "    feature_x = layers.Dropout(0.5)(feature_x)\n",
        "    feature_x = layers.Dense(128, activation='swish')(feature_x)\n",
        "    feature_x = layers.Dropout(0.5)(feature_x)\n",
        "    feature_x = layers.Dense(64, activation='swish')(feature_x)\n",
        "    \n",
        "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    output = layers.Dense(1)(x)\n",
        "    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n",
        "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
        "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
        "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T18:00:55.582258Z",
          "iopub.execute_input": "2022-03-05T18:00:55.582919Z",
          "iopub.status.idle": "2022-03-05T18:00:55.614923Z",
          "shell.execute_reply.started": "2022-03-05T18:00:55.582879Z",
          "shell.execute_reply": "2022-03-05T18:00:55.613955Z"
        },
        "trusted": true,
        "id": "yCYj36U6QLk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#del train\n",
        "#del investment_id\n",
        "del y\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T18:00:55.616561Z",
          "iopub.execute_input": "2022-03-05T18:00:55.61689Z",
          "iopub.status.idle": "2022-03-05T18:00:55.961574Z",
          "shell.execute_reply.started": "2022-03-05T18:00:55.616848Z",
          "shell.execute_reply": "2022-03-05T18:00:55.960857Z"
        },
        "trusted": true,
        "id": "TzaN3UO5QLk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the 3 model gotten to the list 'models' for running an inference on the trained model and finally makig the prediction on the hidden test set using the kaggle api given"
      ],
      "metadata": {
        "id": "me85ov6KQLk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "for i in range(5):\n",
        "    model = get_model()\n",
        "    model.load_weights(f'../input/dnn-base/model_{i}')\n",
        "    models.append(model)\n",
        "for i in range(10):\n",
        "    model = get_model2()\n",
        "    model.load_weights(f'../input/train-dnn-v2-10fold/model_{i}')\n",
        "    models.append(model)\n",
        "    \n",
        "    \n",
        "for i in range(10):\n",
        "    model = get_model3()\n",
        "    model.load_weights(f'../input/dnnmodelnew/model_{i}')\n",
        "    models.append(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T18:00:55.963055Z",
          "iopub.execute_input": "2022-03-05T18:00:55.96333Z",
          "iopub.status.idle": "2022-03-05T18:00:57.317093Z",
          "shell.execute_reply.started": "2022-03-05T18:00:55.963292Z",
          "shell.execute_reply": "2022-03-05T18:00:57.315134Z"
        },
        "trusted": true,
        "id": "2tCmrZNYQLk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_test(investment_id, feature):\n",
        "    return (investment_id, feature), 0\n",
        "def make_test_dataset(feature, investment_id, batch_size=1024):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n",
        "    ds = ds.map(preprocess_test)\n",
        "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "def inference(models, ds):\n",
        "    y_preds = []\n",
        "    for model in models:\n",
        "        y_pred = model.predict(ds)\n",
        "        y_preds.append(y_pred)\n",
        "    return np.mean(y_preds, axis=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T18:00:57.317941Z",
          "iopub.status.idle": "2022-03-05T18:00:57.318264Z",
          "shell.execute_reply.started": "2022-03-05T18:00:57.318105Z",
          "shell.execute_reply": "2022-03-05T18:00:57.318125Z"
        },
        "trusted": true,
        "id": "wdipZS5FQLk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ubiquant\n",
        "env = ubiquant.make_env()\n",
        "iter_test = env.iter_test() \n",
        "for (test_df, sample_prediction_df) in iter_test:\n",
        "    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n",
        "    sample_prediction_df['target'] = inference(models, ds)\n",
        "    env.predict(sample_prediction_df) \n",
        "    display(sample_prediction_df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-05T18:00:57.319948Z",
          "iopub.status.idle": "2022-03-05T18:00:57.320404Z",
          "shell.execute_reply.started": "2022-03-05T18:00:57.320174Z",
          "shell.execute_reply": "2022-03-05T18:00:57.320199Z"
        },
        "trusted": true,
        "id": "HcGHMFrPQLk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "This was a complex project to tackle from the beginninng, given the large dataset and its accompanying memory issues. The EDA revealed some interesting observations such as about a quarter of the data (mostly of the features) has less than half the amount of unique values compared to the other features suggesting that some features might be more categorical than initially thought of. Moreover, it was found that some gaps in the data could be correlated to the 2015 Chinese bubble burst. \n",
        "\n",
        "Four different order reduction methods were explored, namely PCA, LGBM, Random Forest, and XGBoost. \n",
        "\n",
        "Ensemble model is incomplete, we planned to have an ensemble based on models from different approaches, including gradient boosting, decision trees, and deep neural networks.\n",
        "\n",
        "The best kaggle score achieved was of 0.153 "
      ],
      "metadata": {
        "id": "GEsxmOzsQLk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5CwU0TAuQLk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendations for future works\n",
        "Since the main constraint was time, several options could not be explored. In the future, the following options can be considered for further potential improvements in the results:\n",
        "* Better selection of features\n",
        "* Better preprocessing\n",
        "* More attention to the time series nature of the dataset\n",
        "* Hyperparameter finetuning\n",
        "* Better ensemble model. Adding weights to the contribution of each model? Possibility of finetuning of weights"
      ],
      "metadata": {
        "id": "HFlh8dTAQLk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5bul8PxqQLk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References \n",
        "\n",
        "[1] TORCH ME. 'The most advanced analytics' (2022). Kaggle notebook URL: https://www.kaggle.com/kartushovdanil/the-most-advanced-analytics\n",
        "\n",
        "[2] VALLEY. 'Ubiquant LGBM Baseline' (2022).Kaggle notebook URL: https://www.kaggle.com/valleyzw/ubiquant-lgbm-baseline\n",
        "\n",
        "[3] GUNES EVITAN. 'Ubiquant Market Prediction - EDA' (2022). Kaggle notebook URL: https://www.kaggle.com/gunesevitan/ubiquant-market-prediction-eda"
      ],
      "metadata": {
        "id": "ucrN1iMpQLk5"
      }
    }
  ]
}